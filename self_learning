#!/usr/bin/env python

# ==========================
# ENV
# ==========================
import os

os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0")
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["UNSLOTH_COMPILE_DISABLE"] = "1"
os.environ["HF_HOME"] = "./hf_cache"

# ==========================
# IMPORTS
# ==========================
import argparse
import gc
import io
import json
import random
import re
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Iterator, List, Any, Optional

import torch
from PIL import Image as PILImage
from tqdm import tqdm

from datasets import Dataset

from unsloth import FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

# ==========================
# TOKENS
# ==========================
REASONING_START = "<reasoning>"
REASONING_END   = "</reasoning>"
SOLUTION_START  = "<solution>"
SOLUTION_END    = "</solution>"

MAX_IMAGE_SIDE = 768


# ==========================
# CONFIG
# ==========================
@dataclass
class AlgoConfig:
    base_model:           str   = "unsloth/Qwen3-VL-8B-Instruct"
    iterations_k:         int   = 3
    step_size_j:          int   = 500
    max_seq_length:       int   = 2048
    learning_rate:        float = 2e-4
    epochs_per_iter:      int   = 1
    batch_size:           int   = 1
    grad_accum:           int   = 8
    warmup_steps:         int   = 5
    gen_max_new_tokens:   int   = 256
    cls_max_new_tokens:   int   = 16
    force_float_solution: bool  = True
    out_dir:              str   = "outputs"


# ==========================
# IMAGE HELPERS
# ==========================
def resize_if_needed(img: PILImage.Image) -> PILImage.Image:
    w, h = img.size
    m = max(w, h)
    if m <= MAX_IMAGE_SIDE:
        return img
    s  = MAX_IMAGE_SIDE / float(m)
    nw = max(1, int(w * s))
    nh = max(1, int(h * s))
    return img.resize((nw, nh))


def to_pil_rgb(x: Any) -> PILImage.Image:
    if isinstance(x, PILImage.Image):
        return resize_if_needed(x.convert("RGB"))
    if isinstance(x, str):
        return resize_if_needed(PILImage.open(x).convert("RGB"))
    if isinstance(x, dict) and x.get("bytes"):
        return resize_if_needed(
            PILImage.open(io.BytesIO(x["bytes"])).convert("RGB")
        )
    if isinstance(x, dict) and x.get("path"):
        return resize_if_needed(PILImage.open(x["path"]).convert("RGB"))
    raise ValueError(f"Bad image input: {type(x)}")


# ==========================
# DOMAIN STREAM
# ==========================
def resolve_image(domain_jsonl: str, rel: str) -> str:
    base = Path(domain_jsonl).parent
    return rel if os.path.isabs(rel) else str(base / rel)


def domain_iterator(domain_jsonl: str, seed: int) -> Iterator[Dict]:
    rng = random.Random(seed)
    buf = []

    with open(domain_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue

            x = json.loads(line)

            img_rel = str(x.get("file", x.get("image", ""))).strip()
            if not img_rel:
                continue

            try:
                img = to_pil_rgb(resolve_image(domain_jsonl, img_rel))
            except Exception:
                continue

            buf.append(
                {
                    "question": str(x.get("question", "")).strip(),
                    "context":  str(x.get("context",  "")).strip(),
                    "answer":   str(x.get("answer",   "")).strip(),
                    "file":     img_rel,
                    "image":    img,
                }
            )

            if len(buf) >= 256:
                rng.shuffle(buf)
                while buf:
                    yield buf.pop()

    rng.shuffle(buf)
    while buf:
        yield buf.pop()


# ==========================
# PARSING  ← ALL FIXES HERE
# ==========================

# Compiled once — DOTALL + strip whitespace inside tags
_REASONING_RE = re.compile(
    re.escape(REASONING_START) + r"\s*(.*?)\s*" + re.escape(REASONING_END),
    re.DOTALL,
)

_SOL_RE = re.compile(
    re.escape(SOLUTION_START) + r"\s*(.*?)\s*" + re.escape(SOLUTION_END),
    re.DOTALL,
)

_FLOAT_RE = re.compile(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?")


def _strip_tags(text: str) -> str:
    """Remove any leftover XML-style tags from a string."""
    return re.sub(r"<[^>]+>", "", text).strip()


def extract_reasoning_block(text: str) -> str:
    """
    Return the content between <reasoning>...</reasoning>.
    Strips any accidentally included <solution> block from the result.
    Falls back to empty string.
    """
    m = _REASONING_RE.search(text or "")
    if not m:
        return ""
    rationale = m.group(1).strip()
    # Remove any ... that leaked into the reasoning block
    rationale = _SOL_RE.sub("", rationale).strip()
    return rationale


def extract_solution_block(text: str) -> Optional[str]:
    """
    Return the content between ..., fully sanitised.
    Returns None when the tag pair is absent or the content is empty.
    """
    m = _SOL_RE.search(text or "")
    if not m:
        return None
    val = m.group(1).strip()
    # Belt-and-suspenders: nuke any leftover tags (e.g. stray </solution>)
    val = _strip_tags(val)
    return val if val else None


def extract_single_float(text: str) -> Optional[str]:
    m = _FLOAT_RE.search(text or "")
    return m.group(0) if m else None


def parse_model_answer(raw_text: str, force_float: bool) -> str:
    """
    Primary extraction path used by mg_generate.
    1. Try <solution>…</solution>
    2. If force_float, try to pull a bare float from the whole output.
    3. Return whatever we found, fully tag-stripped.
    """
    raw_text = (raw_text or "").strip()

    sol = extract_solution_block(raw_text)
    if sol is not None:
        if force_float:
            f = extract_single_float(sol)
            return f if f is not None else sol
        return sol

    # No solution tags found
    if force_float:
        f = extract_single_float(raw_text)
        return f if f is not None else _strip_tags(raw_text)

    return _strip_tags(raw_text)


# ==========================
# PROMPTS
# ==========================
def build_gen_prompt(ex: Dict, force_float: bool = False) -> str:
    q = str(ex.get("question", "")).strip()
    c = str(ex.get("context",  "")).strip()

    q_text = f"Context: {c}\n\nQuestion: {q}" if c else q

    if force_float:
        solution_rule = (
            f"and then your final answer between {SOLUTION_START} and "
            f"{SOLUTION_END}. Put a single float inside "
            f"{SOLUTION_START}{SOLUTION_END}."
        )
    else:
        solution_rule = (
            f"and then your final answer between {SOLUTION_START} and "
            f"{SOLUTION_END}. Put only the final answer inside "
            f"{SOLUTION_START}{SOLUTION_END}."
        )

    return (
        f"{q_text}\n"
        f"Also first provide your reasoning or working out on how you "
        f"would go about solving the question "
        f"between {REASONING_START} and {REASONING_END} "
        f"{solution_rule}"
    )


def build_cls_prompt(ex: Dict) -> str:
    q        = str(ex.get("question",    "")).strip()
    c        = str(ex.get("context",     "")).strip()
    gold     = str(ex.get("answer",      "")).strip()
    proposed = str(ex.get("generated_c", "")).strip()

    return (
        "You are a vision language model that checks answers for questions "
        "about patent images.\n"
        "Task: Decide if the proposed answer is correct or incorrect.\n"
        "Rules:\n"
        "1. Look at the question, context, image, and the ground truth answer.\n"
        "2. Compare the proposed answer with the ground truth answer in meaning "
        "and detail.\n"
        "3. Reply with exactly one word: correct or incorrect.\n"
        "4. Do not add any extra words or punctuation.\n"
        f"Context: {c}\n"
        f"Question: {q}\n"
        f"Ground truth answer: {gold}\n"
        f"Proposed answer: {proposed}\n"
        "Reply with only one word:"
    )


# ==========================
# FORMATTERS
# ==========================
def format_for_gen(ex: Dict, cfg: AlgoConfig) -> Dict:
    user_text = build_gen_prompt(ex, force_float=cfg.force_float_solution)

    gold = str(ex.get("answer", "")).strip()
    if cfg.force_float_solution:
        f    = extract_single_float(gold)
        gold = f if f is not None else gold

    rationale = str(ex.get("generated_rationale", "")).strip()

    assistant_text = (
        f"{REASONING_START}{rationale}{REASONING_END}\n"
        f"{SOLUTION_START}{gold}{SOLUTION_END}"
    )

    return {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": ex["image"]},
                    {"type": "text",  "text":  user_text},
                ],
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": assistant_text}],
            },
        ]
    }


def format_for_cls(ex: Dict) -> Dict:
    user_text = build_cls_prompt(ex)
    label     = str(ex.get("answer_correct", "")).strip()

    return {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": ex["image"]},
                    {"type": "text",  "text":  user_text},
                ],
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": label}],
            },
        ]
    }


# ==========================
# MODEL MANAGER
# ==========================
class DualModelManager:
    def __init__(self, cfg: AlgoConfig):
        self.cfg = cfg

        print(f"Loading Base Model: {cfg.base_model}")
        self.model, self.tokenizer = FastVisionModel.from_pretrained(
            cfg.base_model,
            load_in_4bit=True,
            use_gradient_checkpointing="unsloth",
        )

        self.gen_path = Path(cfg.out_dir) / "generator_adapter"
        self.cls_path = Path(cfg.out_dir) / "classifier_adapter"

        self._init_adapter(str(self.gen_path))
        self._init_adapter(str(self.cls_path))

    def _init_adapter(self, path: str):
        if not os.path.exists(path):
            print(f"Initializing fresh adapter at {path}")
            self.model = FastVisionModel.get_peft_model(
                self.model,
                finetune_vision_layers=True,
                finetune_language_layers=True,
                finetune_attention_modules=True,
                finetune_mlp_modules=True,
                r=16,
                lora_alpha=16,
                lora_dropout=0,
                bias="none",
                random_state=3407,
                use_rslora=False,
            )
            self.model.save_pretrained(path)
            self.tokenizer.save_pretrained(path)
            print(f"Saved initial adapter to {path}")
            try:
                self.model = self.model.unload()
            except Exception:
                pass

    def _clean_active_adapters(self):
        try:
            self.model = self.model.unload()
        except Exception:
            pass
        if hasattr(self.model, "peft_config"):
            for name in list(self.model.peft_config.keys()):
                try:
                    self.model.delete_adapter(name)
                except Exception:
                    pass

    def load_generator(self, inference: bool = False):
        print("Swapping to GENERATOR adapter")
        self._clean_active_adapters()
        self.model.load_adapter(str(self.gen_path), adapter_name="generator")
        self.model.set_adapter("generator")
        if inference:
            FastVisionModel.for_inference(self.model)
        else:
            FastVisionModel.for_training(self.model)

    def load_classifier(self, inference: bool = False):
        print("Swapping to CLASSIFIER adapter")
        self._clean_active_adapters()
        self.model.load_adapter(str(self.cls_path), adapter_name="classifier")
        self.model.set_adapter("classifier")
        if inference:
            FastVisionModel.for_inference(self.model)
        else:
            FastVisionModel.for_training(self.model)


# ==========================
# MG GENERATE → DISK
# ==========================
def mg_generate(manager: DualModelManager, batch: List[Dict], out_path: Path):
    manager.load_generator(True)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("w", encoding="utf-8") as f:
        for ex in tqdm(batch, desc="MG generate"):

            msgs = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {
                            "type": "text",
                            "text": build_gen_prompt(
                                ex,
                                force_float=manager.cfg.force_float_solution,
                            ),
                        },
                    ],
                }
            ]

            input_text = manager.tokenizer.apply_chat_template(
                msgs,
                add_generation_prompt=True,
            )

            inputs = manager.tokenizer(
                ex["image"],
                input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to("cuda")

            with torch.inference_mode():
                out = manager.model.generate(
                    **inputs,
                    max_new_tokens=manager.cfg.gen_max_new_tokens,
                    use_cache=True,
                )

            prompt_len = inputs["input_ids"].shape[1]
            decoded    = manager.tokenizer.decode(
                out[0][prompt_len:],
                skip_special_tokens=True,
            ).strip()

            # ── rationale ──────────────────────────────────────────────────
            rationale = extract_reasoning_block(decoded)
            if not rationale:
                # Model ignored the tags: use everything before <solution> or
                # the whole output if there is no solution tag either.
                before_sol = decoded.split(SOLUTION_START)[0].strip()
                rationale  = _strip_tags(before_sol) if before_sol else _strip_tags(decoded)

            # ── solution ───────────────────────────────────────────────────
            solution = parse_model_answer(
                decoded,
                force_float=manager.cfg.force_float_solution,
            )
            if not solution:
                # Last-resort: grab the final non-empty line, strip tags
                last_line = ""
                for line in reversed(decoded.splitlines()):
                    stripped = _strip_tags(line).strip()
                    if stripped:
                        last_line = stripped
                        break
                solution = last_line

            f.write(
                json.dumps(
                    {
                        "question":            ex["question"],
                        "context":             ex["context"],
                        "answer":              ex["answer"],
                        "file":                ex["file"],
                        "generated_raw":       decoded,
                        "generated_rationale": rationale,
                        "generated_c":         solution,
                    },
                    ensure_ascii=False,
                )
                + "\n"
            )


# ==========================
# MC VALIDATE → DISK
# ==========================
def mc_validate(
    manager:      DualModelManager,
    gen_jsonl:    Path,
    domain_jsonl: str,
    cls_jsonl:    Path,
):
    manager.load_classifier(True)
    gen_items: List[Dict] = []
    cls_items: List[Dict] = []

    with open(gen_jsonl) as fin, open(cls_jsonl, "w", encoding="utf-8") as fout:
        for line in tqdm(fin, desc="MC validate"):
            ex        = json.loads(line)
            ex["image"] = to_pil_rgb(resolve_image(domain_jsonl, ex["file"]))

            msgs = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": build_cls_prompt(ex)},
                    ],
                }
            ]

            input_text = manager.tokenizer.apply_chat_template(
                msgs,
                add_generation_prompt=True,
            )

            inputs = manager.tokenizer(
                ex["image"],
                input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to("cuda")

            with torch.inference_mode():
                out = manager.model.generate(
                    **inputs,
                    max_new_tokens=manager.cfg.cls_max_new_tokens,
                    use_cache=True,
                )

            prompt_len = inputs["input_ids"].shape[1]
            verdict    = manager.tokenizer.decode(
                out[0][prompt_len:],
                skip_special_tokens=True,
            ).strip().lower()

            # Lenient: "correct" present AND "incorrect" absent
            is_correct = ("correct" in verdict) and ("incorrect" not in verdict)

            out_ex = {k: v for k, v in ex.items() if k != "image"}
            out_ex["answer_correct"] = "correct" if is_correct else "incorrect"
            fout.write(json.dumps(out_ex, ensure_ascii=False) + "\n")

            if ex.get("generated_rationale", "").strip():
                if is_correct:
                    gen_items.append(
                        {
                            "question":            ex["question"],
                            "context":             ex["context"],
                            "answer":              ex["generated_c"],
                            "generated_rationale": ex["generated_rationale"],
                            "image":               ex["image"],
                        }
                    )

                cls_items.append(
                    {
                        "question":            ex["question"],
                        "context":             ex["context"],
                        "answer":              ex["generated_c"],
                        "generated_rationale": ex["generated_rationale"],
                        "answer_correct":      "correct" if is_correct else "incorrect",
                        "image":               ex["image"],
                    }
                )

    return gen_items, cls_items


# ==========================
# BUILD DATASETS
# ==========================
def build_gen_dataset(items: List[Dict], cfg: AlgoConfig) -> Dataset:
    return Dataset.from_list([format_for_gen(ex, cfg) for ex in items])


def build_cls_dataset(items: List[Dict]) -> Dataset:
    return Dataset.from_list([format_for_cls(ex) for ex in items])


# ==========================
# FINETUNE
# ==========================
def finetune(manager: DualModelManager, items: List[Dict], mode: str):
    if not items:
        print(f"  [finetune] No items for {mode}, skipping.")
        return

    print(f"  [finetune] Training {mode} on {len(items)} examples …")

    if mode == "generator":
        manager.load_generator(False)
        ds        = build_gen_dataset(items, manager.cfg)
        save_path = manager.gen_path
    else:
        manager.load_classifier(False)
        ds        = build_cls_dataset(items)
        save_path = manager.cls_path

    trainer = SFTTrainer(
        model=manager.model,
        tokenizer=manager.tokenizer,
        data_collator=UnslothVisionDataCollator(manager.model, manager.tokenizer),
        train_dataset=ds,
        args=SFTConfig(
            per_device_train_batch_size    = manager.cfg.batch_size,
            gradient_accumulation_steps    = manager.cfg.grad_accum,
            warmup_steps                   = manager.cfg.warmup_steps,
            num_train_epochs               = manager.cfg.epochs_per_iter,
            learning_rate                  = manager.cfg.learning_rate,
            output_dir                     = os.path.join(manager.cfg.out_dir, "outputs_temp"),
            optim                          = "adamw_8bit",
            seed                           = 3407,
            remove_unused_columns          = False,
            dataset_kwargs                 = {"skip_prepare_dataset": True},
            max_length                     = manager.cfg.max_seq_length,
            logging_steps                  = 10,
            save_steps                     = 200,
            report_to                      = "wandb",
        ),
    )

    trainer.train()

    print(f"  Saving updated {mode} adapter to {save_path}")
    manager.model.save_pretrained(str(save_path))
    manager.tokenizer.save_pretrained(str(save_path))

    del trainer
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


# ==========================
# MAIN
# ==========================
def main():
    p = argparse.ArgumentParser()
    p.add_argument("--domain_data",          required=True,
                   help="JSONL with fields: question, context, answer, file")
    p.add_argument("--iterations_k",         type=int,   default=3)
    p.add_argument("--step_size_j",          type=int,   default=5000)
    p.add_argument("--seed",                 type=int,   default=42)
    p.add_argument("--base_model",           default="unsloth/Qwen3-VL-8B-Instruct")
    p.add_argument("--max_seq_length",       type=int,   default=2048)
    p.add_argument("--learning_rate",        type=float, default=3e-4)
    p.add_argument("--epochs_per_iter",      type=int,   default=1)
    p.add_argument("--batch_size",           type=int,   default=1)
    p.add_argument("--grad_accum",           type=int,   default=8)
    p.add_argument("--gen_max_new_tokens",   type=int,   default=1028)
    p.add_argument("--cls_max_new_tokens",   type=int,   default=16)
    p.add_argument("--force_float_solution", action="store_true")
    p.add_argument("--out_dir",              default="outputs")
    args = p.parse_args()

    cfg = AlgoConfig(
        base_model           = args.base_model,
        iterations_k         = args.iterations_k,
        step_size_j          = args.step_size_j,
        max_seq_length       = args.max_seq_length,
        learning_rate        = args.learning_rate,
        epochs_per_iter      = args.epochs_per_iter,
        batch_size           = args.batch_size,
        grad_accum           = args.grad_accum,
        gen_max_new_tokens   = args.gen_max_new_tokens,
        cls_max_new_tokens   = args.cls_max_new_tokens,
        force_float_solution = args.force_float_solution,
        out_dir              = args.out_dir,
    )

    random.seed(args.seed)
    Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)

    stream  = domain_iterator(args.domain_data, args.seed)
    manager = DualModelManager(cfg)

    for it in range(cfg.iterations_k):
        print(f"\n{'='*50}")
        print(f"  ITERATION {it+1}/{cfg.iterations_k}")
        print(f"{'='*50}")

        batch = []
        for _ in range(cfg.step_size_j):
            try:
                batch.append(next(stream))
            except StopIteration:
                break

        if not batch:
            print("  No more data, stopping.")
            break

        print(f"  Batch size: {len(batch)}")

        gen_p = Path(cfg.out_dir) / f"iter_{it}_gen.jsonl"
        cls_p = Path(cfg.out_dir) / f"iter_{it}_cls.jsonl"

        mg_generate(manager, batch, gen_p)
        gen_items, cls_items = mc_validate(
            manager, gen_p, args.domain_data, cls_p
        )

        print(f"  Generator training samples : {len(gen_items)}")
        print(f"  Classifier training samples: {len(cls_items)}")

        finetune(manager, gen_items, "generator")
        finetune(manager, cls_items, "classifier")

    print("\nDONE")


if __name__ == "__main__":
    main()
