#!/usr/bin/env python

# ==========================
# ENV
# ==========================
import os

os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0")
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["UNSLOTH_COMPILE_DISABLE"] = "1"
os.environ["HF_HOME"] = "./hf_cache"

# ==========================
# IMPORTS
# ==========================
import argparse
import gc
import io
import json
import random
import re
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Iterator, List, Any, Optional

import torch
from PIL import Image as PILImage
from tqdm import tqdm

from datasets import Dataset, Features, Value
from datasets.features import Image as HFImage

from unsloth import FastVisionModel
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig

# ==========================
# TOKENS
# ==========================
REASONING_START = "<reasoning>"
REASONING_END = ""
SOLUTION_START = "<solution>"
SOLUTION_END = ""

MAX_IMAGE_SIDE = 768

# ==========================
# CONFIG
# ==========================
@dataclass
class AlgoConfig:
    base_model: str = "unsloth/Qwen3-VL-8B-Instruct"
    iterations_k: int = 3
    step_size_j: int = 500
    max_seq_length: int = 2048
    learning_rate: float = 2e-4
    epochs_per_iter: int = 1
    batch_size: int = 1
    grad_accum: int = 8
    warmup_steps: int = 5
    gen_max_new_tokens: int = 256
    cls_max_new_tokens: int = 16
    force_float_solution: bool = True
    out_dir: str = "outputs"


# ==========================
# IMAGE HELPERS
# ==========================
def resize_if_needed(img: PILImage.Image) -> PILImage.Image:
    w, h = img.size
    m = max(w, h)
    if m <= MAX_IMAGE_SIDE:
        return img
    s = MAX_IMAGE_SIDE / float(m)
    nw = max(1, int(w * s))
    nh = max(1, int(h * s))
    return img.resize((nw, nh))


def to_pil_rgb(x: Any) -> PILImage.Image:
    if isinstance(x, PILImage.Image):
        return resize_if_needed(x.convert("RGB"))
    if isinstance(x, str):
        return resize_if_needed(PILImage.open(x).convert("RGB"))
    if isinstance(x, dict) and x.get("bytes"):
        return resize_if_needed(
            PILImage.open(io.BytesIO(x["bytes"])).convert("RGB")
        )
    if isinstance(x, dict) and x.get("path"):
        return resize_if_needed(PILImage.open(x["path"]).convert("RGB"))
    raise ValueError(f"Bad image input: {type(x)}")


# ==========================
# DOMAIN STREAM
# ==========================
def resolve_image(domain_jsonl: str, rel: str) -> str:
    base = Path(domain_jsonl).parent
    return rel if os.path.isabs(rel) else str(base / rel)


def domain_iterator(domain_jsonl: str, seed: int) -> Iterator[Dict]:
    rng = random.Random(seed)
    buf = []

    with open(domain_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue

            x = json.loads(line)

            img_rel = str(x.get("file", x.get("image", ""))).strip()
            if not img_rel:
                continue

            try:
                img = to_pil_rgb(resolve_image(domain_jsonl, img_rel))
            except Exception:
                continue

            buf.append(
                {
                    "question": str(x.get("question", "")).strip(),
                    "context": str(x.get("context", "")).strip(),
                    "answer": str(x.get("answer", "")).strip(),
                    "file": img_rel,
                    "image": img,
                }
            )

            if len(buf) >= 256:
                rng.shuffle(buf)
                while buf:
                    yield buf.pop()

    rng.shuffle(buf)
    while buf:
        yield buf.pop()


# ==========================
# PARSING
# ==========================
_SOL_RE = re.compile(
    re.escape(SOLUTION_START) + r"(.*?)" + re.escape(SOLUTION_END),
    re.DOTALL,
)

_FLOAT_RE = re.compile(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?")


def extract_solution_block(text: str) -> Optional[str]:
    m = _SOL_RE.search((text or "").strip())
    if not m:
        return None
    return m.group(1).strip()


def extract_single_float(text: str) -> Optional[str]:
    m = _FLOAT_RE.search((text or "").strip())
    if not m:
        return None
    return m.group(0)


def parse_solution(text: str) -> str:
    m = _SOL_RE.search(text or "")
    return m.group(1).strip() if m else ""


def parse_model_answer(raw_text: str, force_float: bool) -> str:
    raw_text = (raw_text or "").strip()
    sol = extract_solution_block(raw_text)
    if sol is not None:
        if force_float:
            f = extract_single_float(sol)
            return f if f is not None else sol
        return sol
    if force_float:
        f = extract_single_float(raw_text)
        return f if f is not None else raw_text
    return raw_text


# ==========================
# PROMPTS (FIXED — reference style)
# ==========================
def build_gen_prompt(ex: Dict, force_float: bool = False) -> str:
    q = str(ex.get("question", "")).strip()
    c = str(ex.get("context", "")).strip()

    if c:
        q_text = f"Context: {c}\n\nQuestion: {q}"
    else:
        q_text = q

    if force_float:
        solution_rule = (
            f"and then your final answer between {SOLUTION_START} and {SOLUTION_END}. "
            f"Put a single float inside {SOLUTION_START}{SOLUTION_END}."
        )
    else:
        solution_rule = (
            f"and then your final answer between {SOLUTION_START} and {SOLUTION_END}. "
            f"Put only the final answer inside {SOLUTION_START}{SOLUTION_END}."
        )

    return (
        f"{q_text}\n"
        f"Also first provide your reasoning or working out on how you "
        f"would go about solving the question "
        f"between {REASONING_START} and {REASONING_END} "
        f"{solution_rule}"
    )


def build_cls_prompt(ex: Dict) -> str:
    q = str(ex.get("question", "")).strip()
    c = str(ex.get("context", "")).strip()
    gold = str(ex.get("answer", "")).strip()
    proposed = str(ex.get("generated_c", "")).strip()

    return (
        "You are a vision language model that checks answers for questions about patent images.\n"
        "Task: Decide if the proposed answer is correct or incorrect.\n"
        "Rules:\n"
        "1. Look at the question, context, image, and the ground truth answer.\n"
        "2. Compare the proposed answer with the ground truth answer in meaning and detail.\n"
        "3. Reply with exactly one word: correct or incorrect.\n"
        "4. Do not add any extra words or punctuation.\n"
        f"Context: {c}\n"
        f"Question: {q}\n"
        f"Ground truth answer: {gold}\n"
        f"Proposed answer: {proposed}\n"
        "Reply with only one word:"
    )


# ==========================
# FORMATTERS — IMAGE EMBEDDED IN CONTENT
# ==========================
def format_for_gen(ex: Dict, cfg: AlgoConfig) -> Dict:
    user_text = build_gen_prompt(ex, force_float=cfg.force_float_solution)

    gold = str(ex.get("answer", "")).strip()
    if cfg.force_float_solution:
        f = extract_single_float(gold)
        gold = f if f is not None else gold

    rationale = str(ex.get("generated_rationale", "")).strip()

    assistant_text = (
        f"{REASONING_START}{rationale}{REASONING_END}\n"
        f"{SOLUTION_START}{gold}{SOLUTION_END}"
    )

    return {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": ex["image"]},
                    {"type": "text", "text": user_text},
                ],
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": assistant_text}],
            },
        ]
    }


def format_for_cls(ex: Dict) -> Dict:
    user_text = build_cls_prompt(ex)
    label = str(ex.get("answer_correct", "")).strip()

    return {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": ex["image"]},
                    {"type": "text", "text": user_text},
                ],
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": label}],
            },
        ]
    }


# ==========================
# MODEL MANAGER (PEFT SAFE)
# ==========================
class DualModelManager:
    def __init__(self, cfg: AlgoConfig):
        self.cfg = cfg

        print(f"Loading Base Model: {cfg.base_model}")
        self.model, self.tokenizer = FastVisionModel.from_pretrained(
            cfg.base_model,
            load_in_4bit=True,
            use_gradient_checkpointing="unsloth",
            fast_inference = True
        )

        self.gen_path = Path(cfg.out_dir) / "generator_adapter"
        self.cls_path = Path(cfg.out_dir) / "classifier_adapter"

        self._init_adapter(str(self.gen_path))
        self._init_adapter(str(self.cls_path))

    def _init_adapter(self, path: str):
        if not os.path.exists(path):
            print(f"Initializing fresh adapter at {path}")
            self.model = FastVisionModel.get_peft_model(
                self.model,
                finetune_vision_layers=True,
                finetune_language_layers=True,
                finetune_attention_modules=True,
                finetune_mlp_modules=True,
                r=16,
                lora_alpha=16,
                lora_dropout=0,
                bias="none",
                random_state=3407,
                use_rslora=False,
            )
            self.model.save_pretrained(path)
            self.tokenizer.save_pretrained(path)
            print(f"Saved initial adapter to {path}")
            try:
                self.model = self.model.unload()
            except Exception:
                pass

    def _clean_active_adapters(self):
        try:
            self.model = self.model.unload()
        except Exception:
            pass
        if hasattr(self.model, "peft_config"):
            for name in list(self.model.peft_config.keys()):
                try:
                    self.model.delete_adapter(name)
                except Exception:
                    pass

    def load_generator(self, inference: bool = False):
        print("Swapping to GENERATOR adapter")
        self._clean_active_adapters()
        self.model.load_adapter(str(self.gen_path), adapter_name="generator")
        self.model.set_adapter("generator")
        if inference:
            FastVisionModel.for_inference(self.model)
        else:
            FastVisionModel.for_training(self.model)

    def load_classifier(self, inference: bool = False):
        print("Swapping to CLASSIFIER adapter")
        self._clean_active_adapters()
        self.model.load_adapter(str(self.cls_path), adapter_name="classifier")
        self.model.set_adapter("classifier")
        if inference:
            FastVisionModel.for_inference(self.model)
        else:
            FastVisionModel.for_training(self.model)


# ==========================
# MG → DISK (FIXED: tokenizer call + fallback)
# ==========================
def mg_generate(manager: DualModelManager, batch: List[Dict], out_path: Path):
    manager.load_generator(True)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("w", encoding="utf-8") as f:
        for ex in tqdm(batch, desc="MG generate"):
            msgs = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": build_gen_prompt(ex, force_float=manager.cfg.force_float_solution)},
                    ],
                }
            ]

            input_text = manager.tokenizer.apply_chat_template(
                msgs,
                add_generation_prompt=True,
            )

            # FIX: positional args — image first, text second
            inputs = manager.tokenizer(
                ex["image"],
                input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to("cuda")

            with torch.inference_mode():
                out = manager.model.generate(
                    **inputs,
                    max_new_tokens=manager.cfg.gen_max_new_tokens,
                    use_cache=True,
                )

            prompt_len = inputs["input_ids"].shape[1]
            decoded = manager.tokenizer.decode(
                out[0][prompt_len:],
                skip_special_tokens=True,
            ).strip()

            # Extract rationale
            rationale = ""
            if REASONING_START in decoded and REASONING_END in decoded:
                m = re.search(
                    f"{REASONING_START}(.*?){REASONING_END}",
                    decoded, re.DOTALL,
                )
                if m:
                    rationale = m.group(1).strip()

            # Fallback: if model didn't use tags, use entire output as rationale
            if not rationale:
                rationale = decoded.strip()

            # Extract solution
            solution = parse_model_answer(decoded, force_float=manager.cfg.force_float_solution)
            if not solution:
                solution = decoded.strip().split("\n")[-1].strip()

            f.write(
                json.dumps(
                    {
                        "question": ex["question"],
                        "context": ex["context"],
                        "answer": ex["answer"],
                        "file": ex["file"],
                        "generated_raw": decoded,
                        "generated_rationale": rationale,
                        "generated_c": solution,
                    },
                    ensure_ascii=False,
                )
                + "\n"
            )


# ==========================
# MC → DISK (FIXED: tokenizer call + lenient matching)
# ==========================
def mc_validate(
    manager: DualModelManager,
    gen_jsonl: Path,
    domain_jsonl: str,
    cls_jsonl: Path,
):
    manager.load_classifier(True)
    gen_items, cls_items = [], []

    with open(gen_jsonl) as fin, open(
        cls_jsonl, "w", encoding="utf-8"
    ) as fout:
        for line in tqdm(fin, desc="MC validate"):
            ex = json.loads(line)
            ex["image"] = to_pil_rgb(
                resolve_image(domain_jsonl, ex["file"])
            )

            msgs = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": build_cls_prompt(ex)},
                    ],
                }
            ]

            input_text = manager.tokenizer.apply_chat_template(
                msgs,
                add_generation_prompt=True,
            )

            # FIX: positional args — image first, text second
            inputs = manager.tokenizer(
                ex["image"],
                input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to("cuda")

            with torch.inference_mode():
                out = manager.model.generate(
                    **inputs,
                    max_new_tokens=manager.cfg.cls_max_new_tokens,
                    use_cache=True,
                )

            prompt_len = inputs["input_ids"].shape[1]
            verdict = manager.tokenizer.decode(
                out[0][prompt_len:],
                skip_special_tokens=True,
            ).strip().lower()

            # FIX: lenient matching
            is_correct = "correct" in verdict and "incorrect" not in verdict

            out_ex = dict(ex)
            out_ex.pop("image", None)
            out_ex["answer_correct"] = "correct" if is_correct else "incorrect"
            fout.write(json.dumps(out_ex, ensure_ascii=False) + "\n")

            # Always collect if we have a rationale (don't skip on empty)
            if ex["generated_rationale"]:
                if is_correct:
                    gen_items.append(
                        {
                            "question": ex["question"],
                            "context": ex["context"],
                            "answer": ex["generated_c"],
                            "generated_rationale": ex["generated_rationale"],
                            "image": ex["image"],
                        }
                    )

                cls_items.append(
                    {
                        "question": ex["question"],
                        "context": ex["context"],
                        "answer": ex["generated_c"],
                        "generated_rationale": ex["generated_rationale"],
                        "answer_correct": "correct" if is_correct else "incorrect",
                        "image": ex["image"],
                    }
                )

    return gen_items, cls_items


# ==========================
# BUILD DATASET DIRECTLY (NO .map() — avoids serialisation issues)
# ==========================
def build_gen_dataset(items: List[Dict], cfg: AlgoConfig) -> Dataset:
    formatted = []
    for ex in items:
        formatted.append(format_for_gen(ex, cfg))
    return Dataset.from_list(formatted)


def build_cls_dataset(items: List[Dict]) -> Dataset:
    formatted = []
    for ex in items:
        formatted.append(format_for_cls(ex))
    return Dataset.from_list(formatted)


# ==========================
# FINETUNE
# ==========================
def finetune(manager: DualModelManager, items: List[Dict], mode: str):
    if not items:
        print(f"  [finetune] No items for {mode}, skipping.")
        return

    print(f"  [finetune] Training {mode} on {len(items)} examples ...")

    if mode == "generator":
        manager.load_generator(False)
        ds = build_gen_dataset(items, manager.cfg)
        save_path = manager.gen_path
    else:
        manager.load_classifier(False)
        ds = build_cls_dataset(items)
        save_path = manager.cls_path

    trainer = SFTTrainer(
        model=manager.model,
        tokenizer=manager.tokenizer,
        data_collator=UnslothVisionDataCollator(
            manager.model, manager.tokenizer
        ),
        train_dataset=ds,
        args=SFTConfig(
            per_device_train_batch_size=manager.cfg.batch_size,
            gradient_accumulation_steps=manager.cfg.grad_accum,
            warmup_steps=manager.cfg.warmup_steps,
            num_train_epochs=manager.cfg.epochs_per_iter,
            learning_rate=manager.cfg.learning_rate,
            output_dir=os.path.join(manager.cfg.out_dir, "outputs_temp"),
            optim="adamw_8bit",
            seed=3407,
            remove_unused_columns=False,
            dataset_kwargs={"skip_prepare_dataset": True},
            max_length=manager.cfg.max_seq_length,
            logging_steps=10,
            save_steps=200,
            report_to="none",
        ),
    )

    trainer.train()

    print(f"  Saving updated {mode} adapter to {save_path}")
    manager.model.save_pretrained(str(save_path))
    manager.tokenizer.save_pretrained(str(save_path))

    del trainer
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


# ==========================
# MAIN
# ==========================
def main():
    p = argparse.ArgumentParser()
    p.add_argument("--domain_data", required=True,
                   help="Path to JSONL with fields: question, context, answer, file")
    p.add_argument("--iterations_k", type=int, default=3)
    p.add_argument("--step_size_j", type=int, default=5000)
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--base_model", default="unsloth/Qwen3-VL-8B-Instruct")
    p.add_argument("--max_seq_length", type=int, default=2048)
    p.add_argument("--learning_rate", type=float, default=3e-4)
    p.add_argument("--epochs_per_iter", type=int, default=1)
    p.add_argument("--batch_size", type=int, default=1)
    p.add_argument("--grad_accum", type=int, default=8)
    p.add_argument("--gen_max_new_tokens", type=int, default=256)
    p.add_argument("--cls_max_new_tokens", type=int, default=16)
    p.add_argument("--force_float_solution", action="store_true")
    p.add_argument("--out_dir", default="outputs")
    args = p.parse_args()

    cfg = AlgoConfig(
        base_model=args.base_model,
        iterations_k=args.iterations_k,
        step_size_j=args.step_size_j,
        max_seq_length=args.max_seq_length,
        learning_rate=args.learning_rate,
        epochs_per_iter=args.epochs_per_iter,
        batch_size=args.batch_size,
        grad_accum=args.grad_accum,
        gen_max_new_tokens=args.gen_max_new_tokens,
        cls_max_new_tokens=args.cls_max_new_tokens,
        force_float_solution=args.force_float_solution,
        out_dir=args.out_dir,
    )

    random.seed(args.seed)
    Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)

    stream = domain_iterator(args.domain_data, args.seed)
    manager = DualModelManager(cfg)

    for it in range(cfg.iterations_k):
        print(f"\n{'='*50}")
        print(f"  ITERATION {it+1}/{cfg.iterations_k}")
        print(f"{'='*50}")

        batch = []
        for _ in range(cfg.step_size_j):
            try:
                batch.append(next(stream))
            except StopIteration:
                break

        if not batch:
            print("  No more data, stopping.")
            break

        print(f"  Batch size: {len(batch)}")

        gen_p = Path(cfg.out_dir) / f"iter_{it}_gen.jsonl"
        cls_p = Path(cfg.out_dir) / f"iter_{it}_cls.jsonl"

        mg_generate(manager, batch, gen_p)
        gen_items, cls_items = mc_validate(
            manager, gen_p, args.domain_data, cls_p
        )

        print(f"  Generator training samples: {len(gen_items)}")
        print(f"  Classifier training samples: {len(cls_items)}")

        finetune(manager, gen_items, "generator")
        finetune(manager, cls_items, "classifier")

    print("\nDONE")


if __name__ == "__main__":
    main()
