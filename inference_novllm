#!/usr/bin/env python3
# ============================================================
# ENV + CACHE SETUP (MUST BE FIRST)
# ============================================================
import os

os.environ["HF_HOME"] = "/workspace/work/hf_cache"
os.environ["HF_HUB_CACHE"] = "/workspace/work/hf_cache/hub"
os.environ["TRANSFORMERS_CACHE"] = "/workspace/work/hf_cache/transformers"
os.environ["HF_DATASETS_CACHE"] = "/workspace/work/hf_cache/datasets"
os.environ["XDG_CACHE_HOME"] = "/workspace/work/hf_cache"
os.environ["TMPDIR"] = "/workspace/work/tmp"

# Triton / Inductor (CRITICAL)
os.environ["TRITON_CACHE_DIR"] = "/workspace/work/triton_cache"
os.environ["TORCHINDUCTOR_CACHE_DIR"] = "/workspace/work/torchinductor"

# ============================================================
# UNSLOTH MUST BE IMPORTED FIRST
# ============================================================
import unsloth
from unsloth import FastVisionModel

import torch
torch._inductor.config.max_autotune = False
torch._inductor.config.compile_threads = 1

import json
import re
from pathlib import Path
from PIL import Image
from tqdm import tqdm
from transformers import AutoProcessor

# ============================================================
# CONFIG
# ============================================================
MODEL_ID = "unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit"

INPUT_JSONL = "./output_with_model_8b.jsonl"
OUTPUT_JSONL = "./output_with_model_32b.jsonl"

MAX_NEW_TOKENS = 256
TEMPERATURE = 0.2
TOP_P = 0.9

MAX_CONTEXT_CHARS = 1800
MAX_QUESTION_CHARS = 600
MAX_IMAGE_WIDTH = 1024

SYSTEM_PROMPT = (
    "You are a helpful assistant for table question answering.\n"
    "You will be given a table image plus a text context and a question.\n"
    "Answer using the table and context.\n"
    "Return output in this exact format:\n"
    "ANSWER: <short answer>\n"
    "RATIONALE: <brief explanation grounded in the table/context>\n"
)

# ============================================================
# HELPERS
# ============================================================
def clamp_text(s: str, max_chars: int) -> str:
    s = (s or "").strip()
    return s if len(s) <= max_chars else s[:max_chars].rstrip() + "..."

def build_user_text(context: str, question: str) -> str:
    context = clamp_text(context, MAX_CONTEXT_CHARS)
    question = clamp_text(question, MAX_QUESTION_CHARS)
    return (
        "<image>\n"
        f"CONTEXT:\n{context}\n\n"
        f"QUESTION:\n{question}\n"
    )

def parse_answer_rationale(text: str):
    if not text:
        return None, None

    t = re.sub(r"(?is)<think>.*?</think>", " ", text).strip()

    ans, rat = None, None

    m = re.search(r"ANSWER\s*[:\-]\s*(.+)", t, re.I)
    if m:
        ans = m.group(1).strip()

    m = re.search(r"RATIONALE\s*[:\-]\s*(.+)", t, re.I | re.S)
    if m:
        rat = m.group(1).strip()

    if not ans:
        lines = [l.strip() for l in t.splitlines() if l.strip()]
        if lines:
            ans = lines[-1]

    return ans, rat

def load_and_resize_image(path: Path) -> Image.Image:
    img = Image.open(path).convert("RGB")
    if img.width > MAX_IMAGE_WIDTH:
        new_h = int(img.height * (MAX_IMAGE_WIDTH / img.width))
        img = img.resize((MAX_IMAGE_WIDTH, new_h))
    return img

# ============================================================
# LOAD MODEL
# ============================================================
print("Loading model:", MODEL_ID)

model, processor = FastVisionModel.from_pretrained(
    MODEL_ID,
    load_in_4bit=True,
    dtype=torch.float16,
    device_map="auto",
)

FastVisionModel.for_inference(model)
model.eval()

# ============================================================
# MAIN
# ============================================================
def main():
    in_path = Path(INPUT_JSONL)
    out_path = Path(OUTPUT_JSONL)

    model_key = "qwen3_32b_vl"
    ans_key = f"answer_{model_key}"
    rat_key = f"rationale_{model_key}"
    raw_key = f"raw_{model_key}"

    n_lines = sum(1 for _ in in_path.open("r", encoding="utf-8"))

    with in_path.open("r", encoding="utf-8") as fin, \
         out_path.open("w", encoding="utf-8") as fout:

        for line in tqdm(fin, total=n_lines, desc="Running Unsloth inference"):
            item = json.loads(line)

            img_path = item.get("table_image")
            if not img_path or not Path(img_path).exists():
                item[ans_key] = None
                item[rat_key] = None
                item[raw_key] = "ERROR: missing image"
                fout.write(json.dumps(item) + "\n")
                continue

            image = load_and_resize_image(Path(img_path))

            user_text = build_user_text(
                item.get("context", ""),
                item.get("question", ""),
            )

            # ðŸš« NO apply_chat_template â€” THIS IS THE FIX
            prompt = SYSTEM_PROMPT.strip() + "\n\n" + user_text.strip()

            inputs = processor(
                text=prompt,
                images=image,
                return_tensors="pt",
            ).to(model.device)

            # Safety check (optional, but useful)
            assert inputs["input_ids"].shape[1] > 0, "NO IMAGE TOKENS PRESENT"

            with torch.no_grad():
                output_ids = model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    temperature=TEMPERATURE,
                    top_p=TOP_P,
                    do_sample=True,
                )

            decoded = processor.batch_decode(
                output_ids[:, inputs["input_ids"].shape[1]:],
                skip_special_tokens=True,
            )[0].strip()

            ans, rat = parse_answer_rationale(decoded)

            item[ans_key] = ans
            item[rat_key] = rat
            item[raw_key] = decoded

            fout.write(json.dumps(item, ensure_ascii=False) + "\n")

    print("Done:", out_path.resolve())

# ============================================================
if __name__ == "__main__":
    main()
